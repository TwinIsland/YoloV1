{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d905ac6b",
   "metadata": {},
   "source": [
    "## Draw sample picture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d1b1d0a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import models\n",
    "\n",
    "from src.resnet_yolo import resnet50\n",
    "from src.dataset import VocDetectorDataset\n",
    "from src.eval_voc import evaluate\n",
    "from src.predict import predict_image\n",
    "from src.config import VOC_CLASSES, COLORS\n",
    "from kaggle_submission import output_submission_csv\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "415df907",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading saved network from checkpoints/best_detector.pth\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "load_network_path = \"checkpoints/best_detector.pth\" #'checkpoints/best_detector.pth' \n",
    "pretrained = True\n",
    "\n",
    "# use to load a previously trained network\n",
    "if load_network_path is not None:\n",
    "    print('Loading saved network from {}'.format(load_network_path))\n",
    "    net = resnet50().to(device)\n",
    "    net.load_state_dict(torch.load(load_network_path, map_location=torch.device(device)))\n",
    "else:\n",
    "    print('Load pre-trained model')\n",
    "    net = resnet50(pretrained=pretrained).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d7396b2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing dataset\n",
      "Loaded 5011 train images\n",
      "Initializing dataset\n",
      "Loaded 4950 test images\n"
     ]
    }
   ],
   "source": [
    "# YOLO network hyperparameters\n",
    "B = 2  # number of bounding box predictions per cell\n",
    "S = 14  # width/height of network output grid (larger than 7x7 from paper since we use a different network)\n",
    "\n",
    "file_root_train = 'data/VOCdevkit_2007/VOC2007/JPEGImages/'\n",
    "annotation_file_train = 'data/voc2007.txt'\n",
    "\n",
    "train_dataset = VocDetectorDataset(root_img_dir=file_root_train,dataset_file=annotation_file_train,train=True, S=S)\n",
    "train_loader = DataLoader(train_dataset,batch_size=batch_size,shuffle=True,num_workers=2)\n",
    "print('Loaded %d train images' % len(train_dataset))\n",
    "\n",
    "file_root_test = 'data/VOCdevkit_2007/VOC2007test/JPEGImages/'\n",
    "annotation_file_test = 'data/voc2007test.txt'\n",
    "\n",
    "test_dataset = VocDetectorDataset(root_img_dir=file_root_test,dataset_file=annotation_file_test,train=False, S=S)\n",
    "test_loader = DataLoader(test_dataset,batch_size=batch_size,shuffle=False,num_workers=2)\n",
    "print('Loaded %d test images' % len(test_dataset))\n",
    "\n",
    "data = train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "794f479f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from src.predict import predict_frame\n",
    "\n",
    "def process_video(video_path, output_path, net, fps_reduction_factor=2):\n",
    "    \"\"\"\n",
    "    Process a video to perform object detection on its frames.\n",
    "\n",
    "    Parameters:\n",
    "    - video_path: path to the input video file.\n",
    "    - output_path: path to save the processed video file.\n",
    "    - net: object detection network model.\n",
    "    - fps_reduction_factor: factor to reduce FPS by, for speeding up processing.\n",
    "    \"\"\"\n",
    "    # Load the video\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    if not cap.isOpened():\n",
    "        print(\"Error opening video file.\")\n",
    "        return\n",
    "\n",
    "    # Get video properties\n",
    "    orig_fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "\n",
    "    # Define the codec and create VideoWriter object\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    out = cv2.VideoWriter(output_path, fourcc, orig_fps, (width, height))\n",
    "\n",
    "    frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "\n",
    "    try:\n",
    "        with tqdm(total=frame_count) as pbar:\n",
    "            frame_id = 0\n",
    "            while cap.isOpened():\n",
    "                ret, frame = cap.read()\n",
    "                if ret and frame_id < frame_count:\n",
    "                    # Convert frame to RGB\n",
    "                    frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "                    # Predict and annotate the frame\n",
    "                    result = predict_frame(net, frame)\n",
    "                    for left_up, right_bottom, class_name, prob in result:\n",
    "                        color = COLORS[VOC_CLASSES.index(class_name)]\n",
    "                        cv2.rectangle(frame, left_up, right_bottom, color, 2)\n",
    "                        label = class_name + str(round(prob, 2))\n",
    "                        text_size, baseline = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, 0.4, 1)\n",
    "                        p1 = (left_up[0], left_up[1] - text_size[1])\n",
    "                        cv2.rectangle(frame, (p1[0] - 2 // 2, p1[1] - 2 - baseline), (p1[0] + text_size[0], p1[1] + text_size[1]), color, -1)\n",
    "                        cv2.putText(frame, label, (p1[0], p1[1] + baseline), cv2.FONT_HERSHEY_SIMPLEX, 0.4, (255, 255, 255), 1, 8)\n",
    "\n",
    "                    # Convert back to BGR for opencv video write\n",
    "                    frame = cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)\n",
    "                    out.write(frame)\n",
    "\n",
    "                    pbar.update(1)\n",
    "                    frame_id += 1\n",
    "                else:\n",
    "                    break\n",
    "    finally:\n",
    "        # Release everything if job is finished\n",
    "        cap.release()\n",
    "        out.release()\n",
    "        cv2.destroyAllWindows()\n",
    "    print(\"Video processing completed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1f15ed2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 97%|████████████████████████████████████████████████████████████████████████████▎  | 976/1011 [04:06<00:08,  4.01it/s]"
     ]
    }
   ],
   "source": [
    "process_video('sample.mp4', 'sample_labeled.mp4', net)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "751624c9",
   "metadata": {},
   "source": [
    "## "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
